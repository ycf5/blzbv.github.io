WEBVTT
Kind: captions
Language: en

00:00:00.580 --> 00:00:03.230
Decision Trees are powerful
modelling technique.

00:00:03.230 --> 00:00:06.540
But like all techniques
the algorithm can have its issues.

00:00:06.540 --> 00:00:09.760
Decision Trees are prone to
an error called overfitting

00:00:09.760 --> 00:00:12.750
where the model fits the sample
data a little too well and

00:00:12.750 --> 00:00:16.810
as a result does not predict future
results as well so it should.

00:00:16.810 --> 00:00:21.060
A technique that helps eliminate
this issue is a Random Forest Model.

00:00:21.060 --> 00:00:24.910
Let's go over a little background
of what a forest model actually is,

00:00:24.910 --> 00:00:28.870
to build out a forest model, let us
start by building a decision tree.

00:00:28.870 --> 00:00:32.630
If you recall, the decision tree
is constructed by looking for

00:00:32.630 --> 00:00:34.900
groups to split, or Data.

00:00:34.900 --> 00:00:38.670
Splits are chosen at places to produce
the largest difference and for this

00:00:38.670 --> 00:00:43.600
example, the largest difference in the
percent of the mode of transportation.

00:00:43.600 --> 00:00:46.930
We will continue down this path
creating different splits of the data

00:00:46.930 --> 00:00:50.210
until adding more splits
no longer adds value

00:00:50.210 --> 00:00:53.580
in predicting the correct
mode of transportation.

00:00:53.580 --> 00:00:57.170
Now what if we made the same tree
with slightly different data,

00:00:57.170 --> 00:00:58.360
would it be exactly the same?

00:00:59.490 --> 00:01:04.470
No, changes in the data could cause the
tree to split at different points, or

00:01:04.470 --> 00:01:05.720
in a different order.

00:01:05.720 --> 00:01:08.300
A forest model creates
hundreds of trees.

00:01:08.300 --> 00:01:12.680
This is call an assemble of decision
trees, where each tree is created by

00:01:12.680 --> 00:01:16.580
different randomly generated
chunks of the original data.

00:01:16.580 --> 00:01:19.980
Then it looks at the results as
a whole to make a prediction.

00:01:19.980 --> 00:01:24.790
So, how does this fix the overfitting
problem we mentioned before?

00:01:24.790 --> 00:01:29.390
Well each individual tree created
still has the overfitting issues, but

00:01:29.390 --> 00:01:31.360
when you look at the result as a whole.

00:01:31.360 --> 00:01:35.570
The overfitting gets averaged
out by all the other trees.

00:01:35.570 --> 00:01:39.820
The first decision tree will be created
with a subset of the original data.

00:01:39.820 --> 00:01:44.140
We'll also use a different combination
of predictor variables to assist with

00:01:44.140 --> 00:01:46.350
the splits along the trees.

00:01:46.350 --> 00:01:50.350
The second decision tree will
then do the exact same thing

00:01:50.350 --> 00:01:54.410
with a different subset of data
with different predictor variables

00:01:54.410 --> 00:01:57.070
being used to help with
the splits as well.

00:01:57.070 --> 00:02:01.040
This will continue to occur until the
number of decision trees specified has

00:02:01.040 --> 00:02:02.360
been created.

00:02:02.360 --> 00:02:04.750
By default this value
is 500 decision trees.

00:02:06.370 --> 00:02:09.750
Now each employee will
traverse through each tree

00:02:09.750 --> 00:02:11.920
until they reaches a terminal node.

00:02:11.920 --> 00:02:15.480
At each of the terminal nodes,
the decision tree has a vote.

00:02:15.480 --> 00:02:17.950
Car, public transit or bike.

00:02:17.950 --> 00:02:21.410
The terminal node that
is seem most often

00:02:21.410 --> 00:02:24.940
is then classified as the group
that employee falls within.

