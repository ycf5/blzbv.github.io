WEBVTT
Kind: captions
Language: en

00:00:00.490 --> 00:00:04.730
This leads us to the topic of scaling,
or standardizing the data.

00:00:04.730 --> 00:00:07.370
Scaling of the data is
an important consideration

00:00:07.370 --> 00:00:10.650
when getting your data ready for
a clustering process.

00:00:10.650 --> 00:00:15.480
Since the distance concept is important
to determining clusters, the relative

00:00:15.480 --> 00:00:20.730
size of the values between the variables
used becomes important as well.

00:00:20.730 --> 00:00:24.500
As an example, assume we have
two variables we're using for

00:00:24.500 --> 00:00:26.300
a cluster analysis.

00:00:26.300 --> 00:00:30.450
One is a percentage with
a range of 0%- 100%.

00:00:30.449 --> 00:00:34.879
Say of a demographic,
like a percentage of a given ethnicity.

00:00:34.880 --> 00:00:40.469
The other variable is a measure of some
sort of absolute data, say, population.

00:00:40.469 --> 00:00:45.960
Which can go from 0 to any maximum,
but may range into the millions.

00:00:45.960 --> 00:00:50.299
Using these two variables without any
adjustment will give a much higher

00:00:50.299 --> 00:00:54.679
weight to the population variable
than the percentage variable.

00:00:54.679 --> 00:00:58.460
Because clustering solutions are very
sensitive to this kind of difference in

00:00:58.460 --> 00:01:01.390
scale, many algorithms provide for

00:01:01.390 --> 00:01:05.829
automatically standardizing
the variables to similar scales.

00:01:05.829 --> 00:01:10.579
One common method is calculating
the Z-score of each value in the field.

00:01:10.579 --> 00:01:13.439
This is also called the standard score.

00:01:13.439 --> 00:01:17.670
It basically measures the number
of standard deviations each value

00:01:17.670 --> 00:01:19.340
is from the mean.

00:01:19.340 --> 00:01:23.590
Another scaling method is to
calculate the unit interval,

00:01:23.590 --> 00:01:27.677
which compresses all values
in a field to a 0 to 1 range.

00:01:27.677 --> 00:01:31.779
A common method for
this would be to shift the variables so

00:01:31.778 --> 00:01:33.950
that the minimum value is 0.

00:01:33.950 --> 00:01:37.710
So in a case where the minimum
of the original variable is 10,

00:01:37.709 --> 00:01:40.429
you would subtract 10
from each variable.

00:01:41.530 --> 00:01:45.560
Then you would divide each row
in this field by the maximum

00:01:45.560 --> 00:01:46.820
of the previous result.

00:01:47.909 --> 00:01:51.159
So for our example of using
a percentage of ethnicity and

00:01:51.159 --> 00:01:56.319
an absolute population number, when you
use either of these scaling methods,

00:01:56.319 --> 00:01:59.069
the resulting values
can end up being used

00:01:59.069 --> 00:02:03.579
without overweighting the analysis
with the larger population number.

00:02:03.579 --> 00:02:06.179
So this eliminates
the bias in the algorithm

00:02:06.180 --> 00:02:07.900
towards that bigger population number.

