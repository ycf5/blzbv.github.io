WEBVTT
Kind: captions
Language: en

00:00:00.430 --> 00:00:03.700
While both of these methods are based
on the concept of distance,

00:00:03.700 --> 00:00:07.220
which we've covered earlier,
they approach distance differently.

00:00:08.679 --> 00:00:13.239
With the hierarchical method it is more
about determining the closest distance

00:00:13.240 --> 00:00:14.439
between entities.

00:00:15.449 --> 00:00:18.410
Whereas with the K-Centroid
method it is related

00:00:18.410 --> 00:00:21.219
to finding the objects that
are closest to the centroid.

00:00:22.780 --> 00:00:27.690
And while the hierarchical method maybe
easier to explain with a dendrogram

00:00:27.690 --> 00:00:32.520
visualization, it usually works
better on a small set of objects and

00:00:32.520 --> 00:00:37.370
can be slow with a larger set of objects
due to the large number of merge or

00:00:37.369 --> 00:00:39.259
split decisions that it makes.

00:00:40.500 --> 00:00:45.740
The K-Centroid, on the other hand, works
well with a larger set of objects and

00:00:45.740 --> 00:00:48.770
tends to be more efficient in
creating the clustering solution.

00:00:49.869 --> 00:00:54.689
The K-Centroid method works well
with rounded sets of data or

00:00:54.689 --> 00:00:57.024
where the clusters are equal
sizes in densities.

00:00:58.859 --> 00:01:03.289
Where it doesn't work well is where
the clusters have unusual shapes or

00:01:03.289 --> 00:01:05.569
there are differing
densities among the data.

00:01:07.849 --> 00:01:09.559
As we can see in the examples here,

00:01:09.560 --> 00:01:14.007
in the top set of data, the clusters
should really be the two half circles.

00:01:14.007 --> 00:01:18.090
But the K-Centroid process would
break up the half circles, and

00:01:18.090 --> 00:01:21.670
assign some of the points to
the wrong clustered grouping.

00:01:21.670 --> 00:01:25.420
In the bottom example, even though
the left set of points are sparse in

00:01:25.420 --> 00:01:29.341
density, it seems like they should
be all included in the same group.

00:01:29.341 --> 00:01:33.189
The K-Centroid process here could end up

00:01:33.189 --> 00:01:37.609
combining the two right-hand segments
because they're so close together and

00:01:37.609 --> 00:01:40.530
split the left set of
points into two segments.

00:01:42.060 --> 00:01:46.969
In K-centroid models, the location of
the initial centroids, known as the seed

00:01:46.969 --> 00:01:51.118
centroids can be critical to
the quality of the resulting clusters.

00:01:52.200 --> 00:01:56.680
If there is bad seeding, it can affect
the performance of the process and

00:01:56.680 --> 00:02:00.070
potentially give you
bad overall clustering.

00:02:00.069 --> 00:02:04.969
Most clustering software will
automatically choose the initial seeds.

00:02:04.969 --> 00:02:07.950
Now one software solution
that I've used in the past

00:02:07.950 --> 00:02:12.810
basically just used the first objects
in the record order as the seeds.

00:02:12.810 --> 00:02:15.719
And what I found is that
the cluster solution would

00:02:15.719 --> 00:02:20.359
actually change depending on how
I sorted the same set of data.

00:02:20.360 --> 00:02:25.290
So what is a better solution is if the
software provides for multiple starting

00:02:25.289 --> 00:02:29.579
seeds that are chosen at random and
then have the results averaged.

00:02:31.090 --> 00:02:34.360
Something else to keep in mind
with the K-Centroid method

00:02:34.360 --> 00:02:37.420
is that it can be highly
sensitive to outliers.

00:02:37.419 --> 00:02:38.639
If you think about it,

00:02:38.639 --> 00:02:43.269
if you have object that is far away
from the rest of the cluster objects but

00:02:43.270 --> 00:02:47.250
needs to be included in that
cluster because it's the closest.

00:02:47.250 --> 00:02:51.000
Then it will effect where
the center of that cluster is, and

00:02:51.000 --> 00:02:53.539
potentially change
an assignment of clusters

00:02:53.539 --> 00:02:57.150
on the other side of the dividing
point for that cluster.

00:02:58.169 --> 00:03:01.199
This is one reason,
as we've already mentioned,

00:03:01.199 --> 00:03:04.849
that it is better to standardize or
scale the fields.

00:03:04.849 --> 00:03:08.699
This scaling helps eliminate
the effective outliers.

00:03:08.699 --> 00:03:12.849
Also as we will see in our exercise,
there are different methods within

00:03:12.849 --> 00:03:16.430
the K-Centroid processes that
can account for outliers.

